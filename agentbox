#!/usr/bin/env bash
# AgentBox - Simplified Docker environment for Claude development
# Automatically rebuilds when Dockerfile changes and uses ephemeral containers with persistent caches

set -euo pipefail

# Configuration
# Resolve symlinks to get the real script location (allows installation as symlink)
readonly SCRIPT_PATH="$(readlink -f "${BASH_SOURCE[0]}")"
readonly SCRIPT_DIR="$(cd "$(dirname "$SCRIPT_PATH")" && pwd)"
readonly PROJECT_DIR="$(pwd)"
readonly PROJECT_NAME="$(basename "$PROJECT_DIR")"
readonly DOCKERFILE_PATH="${SCRIPT_DIR}/Dockerfile"
readonly ENTRYPOINT_PATH="${SCRIPT_DIR}/entrypoint.sh"
readonly IMAGE_NAME="agentbox:latest"
readonly CONTAINER_PREFIX="agentbox"

# Colors for output
readonly RED='\033[0;31m'
readonly GREEN='\033[0;32m'
readonly YELLOW='\033[1;33m'
readonly BLUE='\033[0;34m'
readonly CYAN='\033[0;36m'
readonly NC='\033[0m' # No Color

# Helper functions
log_info() { echo -e "${BLUE}â„¹${NC}  $1"; }
log_success() { echo -e "${GREEN}âœ…${NC} $1"; }
log_warning() { echo -e "${YELLOW}âš ${NC}  $1"; }
log_error() { echo -e "${RED}âŒ${NC} $1" >&2; }
log_build() { echo -e "${CYAN}ðŸ”¨${NC} $1"; }

# Check if Docker is installed and running
check_docker() {
    if ! command -v docker &> /dev/null; then
        log_error "Docker is not installed. Please install Docker first."
        exit 1
    fi

    if ! docker info &> /dev/null; then
        log_error "Docker daemon is not running. Please start Docker."
        exit 1
    fi
}

# Calculate hash for a file
calculate_hash() {
    local file="$1"
    if [[ -f "$file" ]]; then
        sha256sum "$file" | cut -d' ' -f1
    else
        echo "none"
    fi
}

get_container_name() {
    local project_hash=$(echo -n "$PROJECT_DIR" | sha256sum | cut -c1-12)
    echo "${CONTAINER_PREFIX}-${project_hash}"
}

has_base_instance() {
    local base_name=$(get_container_name)
    local running_containers=$(docker ps --format '{{.Names}}')
    echo "$running_containers" | grep -q "^${base_name}$"
}

find_next_instance_number() {
    local base_name=$(get_container_name)
    local running_containers=$(docker ps --format '{{.Names}}')
    local instance=2
    while echo "$running_containers" | grep -q "^${base_name}-${instance}$"; do
        ((instance++))
    done
    echo "$instance"
}

get_next_instance() {
    has_base_instance || return
    echo "-$(find_next_instance_number)"
}


# Check if image needs rebuild
needs_rebuild() {
    # Calculate current Dockerfile and entrypoint hashes
    local dockerfile_hash=$(calculate_hash "$DOCKERFILE_PATH")
    local entrypoint_hash=$(calculate_hash "$ENTRYPOINT_PATH")
    local combined_hash="${dockerfile_hash}-${entrypoint_hash}"

    # Check if image exists
    if ! docker image inspect "$IMAGE_NAME" &> /dev/null; then
        return 0  # Needs rebuild - image doesn't exist
    fi

    # Get stored hash from image
    local stored_hash=$(docker inspect "$IMAGE_NAME" \
        --format '{{ index .Config.Labels "agentbox.hash" }}' 2>/dev/null || echo "none")

    # Compare hashes
    if [[ "$combined_hash" != "$stored_hash" ]]; then
        return 0  # Needs rebuild
    fi

    return 1  # No rebuild needed
}

# Build Docker image
build_image() {
    local dockerfile_hash=$(calculate_hash "$DOCKERFILE_PATH")
    local entrypoint_hash=$(calculate_hash "$ENTRYPOINT_PATH")
    local combined_hash="${dockerfile_hash}-${entrypoint_hash}"

    log_build "Building AgentBox image (this may take a few minutes on first run)..."

    # Get current user/group IDs
    local user_id=$(id -u)
    local group_id=$(id -g)

    # Build with progress output
    if docker build \
        --build-arg USER_ID="${user_id}" \
        --build-arg GROUP_ID="${group_id}" \
        --build-arg USERNAME="claude" \
        --label "agentbox.hash=${combined_hash}" \
        --label "agentbox.version=1.0.0" \
        --label "agentbox.built=$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
        -t "$IMAGE_NAME" \
        "$SCRIPT_DIR" ; then
        log_success "Image built successfully!"
        return 0
    else
        log_error "Failed to build image"
        return 1
    fi
}

# Cleanup old containers using outdated image
cleanup_old_containers() {
    log_info "Checking for containers using outdated image..."

    # Get the current image ID
    local current_image_id=$(docker inspect "$IMAGE_NAME" --format '{{ .Id }}' 2>/dev/null || echo "none")

    if [[ "$current_image_id" == "none" ]]; then
        return
    fi

    # Find all agentbox containers
    local containers=$(docker ps -a --format '{{.Names}}' | grep "^${CONTAINER_PREFIX}-" || true)

    if [[ -z "$containers" ]]; then
        return
    fi

    local cleaned_count=0

    for container in $containers; do
        local container_image_id=$(docker inspect "$container" --format '{{ .Image }}' 2>/dev/null || echo "none")

        if [[ "$container_image_id" != "$current_image_id" ]]; then
            log_warning "Removing outdated container: $container"
            docker rm -f "$container" &> /dev/null || true
            ((cleaned_count++))
        fi
    done

    if [[ $cleaned_count -gt 0 ]]; then
        log_success "Cleaned up $cleaned_count outdated container(s)"
    fi
}

is_absolute_path() {
    local path="$1"
    [[ "$path" =~ ^/ ]]
}

is_system_path() {
    local path="$1"
    [[ "$path" =~ ^/(etc|var|usr|sys|proc|dev)(/|$) ]]
}

is_duplicate_of_project() {
    local ws_realpath="$1"
    [[ "$ws_realpath" == "$(realpath "$PROJECT_DIR")" ]]
}

is_duplicate_in_array() {
    local ws_realpath="$1"
    local -n check_array_ref=$2

    for existing_ws in "${check_array_ref[@]}"; do
        if [[ "$ws_realpath" == "$existing_ws" ]]; then
            return 0
        fi
    done
    return 1
}

validate_workspace_path() {
    local ws="$1"
    local -n validated_workspaces_ref=$2

    ws=$(echo "$ws" | xargs)

    if [[ "$ws" =~ ^~ ]]; then
        ws="${ws/#\~/$HOME}"
    fi

    if ! is_absolute_path "$ws"; then
        log_error "Workspace must be absolute path: $ws"
        exit 1
    fi

    if [[ ! -d "$ws" ]]; then
        log_error "Workspace directory not found: $ws"
        exit 1
    fi

    local ws_realpath=$(realpath "$ws")

    if is_duplicate_of_project "$ws_realpath"; then
        log_warning "Skipping duplicate: $ws (already mounted as /workspace)"
        return 1
    fi

    if is_duplicate_in_array "$ws_realpath" validated_workspaces_ref; then
        log_warning "Skipping duplicate workspace: $ws"
        return 1
    fi

    if is_system_path "$ws_realpath"; then
        log_warning "Mounting system directory: $ws (proceed with caution)"
    fi

    # Store the realpath, not the original path
    validated_workspaces_ref+=("$ws_realpath")
    return 0
}

parse_workspace_paths() {
    local workspace_string="$1"
    local -n output_array_ref=$2

    IFS=',' read -ra workspace_list <<< "$workspace_string"
    for ws in "${workspace_list[@]}"; do
        validate_workspace_path "$ws" output_array_ref || true
    done
}

# Mount additional workspaces to Docker mount options
mount_additional_workspaces() {
    local -n mount_opts_ref=$1
    shift
    local extra_workspaces=("$@")

    for extra_ws in "${extra_workspaces[@]}"; do
        if [[ -d "$extra_ws" ]]; then
            local folder_name=$(basename "$extra_ws")
            mount_opts_ref+=(-v "$extra_ws:/${folder_name}:z")
            log_info "Mounting additional workspace: $extra_ws -> /${folder_name}"
        else
            log_warning "Skipping non-existent workspace: $extra_ws"
        fi
    done
}

# Run container with --rm (ephemeral)
run_container() {
    local base_container_name="$1"
    local instance_suffix="$2"
    local -n extra_workspaces_ref=$3
    local shell_mode="$4"
    local admin_mode="$5"
    shift 5

    local container_name="${base_container_name}${instance_suffix}"
    local cmd_args=("$@")

    # Build log message
    local log_msg="Starting container for: $PROJECT_NAME"
    if [[ -n "$instance_suffix" ]]; then
        local running_instances=$(docker ps --format '{{.Names}}' | grep -c "^${base_container_name}" || echo "0")
        local total_instances=$((running_instances + 1))
        log_msg+=" [instance ${instance_suffix#-} of $total_instances]"
    fi
    [[ "$admin_mode" == "true" ]] && log_msg+=" [admin]"
    log_success "$log_msg"

    # Prepare mount options
    local mount_opts=(
        -v "$PROJECT_DIR:/workspace:z"
    )

    mount_additional_workspaces mount_opts "${extra_workspaces_ref[@]}"

    # Mount .gitconfig if it exists
    if [[ -f "${HOME}/.gitconfig" ]]; then
        mount_opts+=(-v "${HOME}/.gitconfig:/home/claude/.gitconfig:ro")
    fi

    # Mount dedicated AgentBox SSH directory
    local agentbox_ssh="${HOME}/.agentbox/ssh"
    if [[ -d "${agentbox_ssh}" ]]; then
        mount_opts+=(-v "${agentbox_ssh}:/home/claude/.ssh:rw")
        log_info "AgentBox SSH directory mounted (read-write)"
    else
        log_warning "SSH not configured. Run 'agentbox ssh-init' to enable SSH operations."
    fi

    # Mount cache directories for package managers (per instance)
    local cache_dir="${HOME}/.cache/agentbox/${container_name}"
    mkdir -p "${cache_dir}/npm" "${cache_dir}/pip" "${cache_dir}/maven" "${cache_dir}/gradle"

    mount_opts+=(
        -v "${cache_dir}/npm:/home/claude/.npm"
        -v "${cache_dir}/pip:/home/claude/.cache/pip"
        -v "${cache_dir}/maven:/home/claude/.m2"
        -v "${cache_dir}/gradle:/home/claude/.gradle"
    )

    # Mount per-project persistent data (per instance)
    local project_data_dir="${HOME}/.agentbox/projects/${container_name}"
    local history_dir="${project_data_dir}/history"
    mkdir -p "${history_dir}"

    # Create history files if they don't exist
    touch "${history_dir}/zsh_history"
    touch "${history_dir}/bash_history"

    mount_opts+=(
        -v "${history_dir}/zsh_history:/home/claude/.zsh_history"
        -v "${history_dir}/bash_history:/home/claude/.bash_history"
    )

    # Use Docker named volume for Claude config (SHARED across all instances)
    # This ensures all instances share authentication
    local claude_volume_name="agentbox-claude-${base_container_name#agentbox-}"

    # Check if volume exists and initialize if needed
    if ! docker volume inspect "$claude_volume_name" &>/dev/null; then
        log_info "Creating Claude CLI volume and copying authentication from global config"
        docker volume create "$claude_volume_name" &>/dev/null

        # Initialize volume with global Claude config if it exists
        if [[ -d "${HOME}/.claude" ]]; then
            # Use a temporary container to copy data to the volume
            docker run --rm \
                -v "${HOME}/.claude:/source:ro" \
                -v "${claude_volume_name}:/dest" \
                "$IMAGE_NAME" \
                sh -c "cp -r /source/* /dest/ 2>/dev/null || true"
        fi
    fi

    mount_opts+=(-v "${claude_volume_name}:/home/claude/.claude")

    # Set Claude config directory environment variable
    mount_opts+=(--env "CLAUDE_CONFIG_DIR=/home/claude/.claude")

    if [[ -n "$instance_suffix" ]]; then
        log_info "Claude CLI configuration mounted (shared with other instances - no re-auth needed)"
    else
        log_info "Claude CLI configuration mounted"
    fi

    # Prepare the command to run
    local container_cmd
    if [[ "$shell_mode" == "true" ]]; then
        if [[ "$admin_mode" == "true" ]]; then
            container_cmd=(bash -c "echo 'ðŸ”’ Admin shell - sudo access enabled' && exec ${cmd_args[*]:-/bin/zsh}")
        else
            if [[ ${#cmd_args[@]} -gt 0 ]]; then
                container_cmd=(/bin/zsh "${cmd_args[@]}")
            else
                container_cmd=(/bin/zsh)
            fi
        fi
    else
        # Run claude through zsh to get proper environment
        # Always include --dangerously-skip-permissions, append any additional flags
        local claude_cmd="claude --dangerously-skip-permissions"
        if [[ ${#cmd_args[@]} -gt 0 ]]; then
            claude_cmd="$claude_cmd ${cmd_args[*]}"
        fi
        container_cmd=(zsh -c "source ~/.zshrc && exec $claude_cmd")
    fi

    # Check for .env file in project directory and add to docker args
    local env_file_args=()
    if [[ -f "$PROJECT_DIR/.env" ]]; then
        env_file_args+=(--env-file "$PROJECT_DIR/.env")
        log_info ".env file found and will be loaded into container"
    fi

    # Run ephemeral container with --rm
    docker run -it --rm \
        --name "$container_name" \
        --hostname "agentbox-$PROJECT_NAME" \
        "${mount_opts[@]}" \
        "${env_file_args[@]}" \
        -w /workspace \
        --init \
        "$IMAGE_NAME" \
        "${container_cmd[@]}"
}

# Show help
show_help() {
    cat << EOF
AgentBox - Simplified Docker environment for Claude development

Usage:
    agentbox [OPTIONS] [COMMAND]

Options:
    -h, --help              Show this help message
    --cleanup               Remove AgentBox image and cached data
    --rebuild               Force rebuild of the image
    --add-dir <paths>       Mount additional directories (comma-separated or multiple flags)

Commands:
    shell [--admin] Start interactive shell instead of Claude CLI
    ssh-init        Initialize SSH directory for AgentBox

    If no command is provided, Claude CLI will be started automatically.
    Use 'shell' command to get a shell instead of Claude CLI.
    Use 'shell --admin' to get a shell with sudo privileges.
    Other commands will be executed inside the container.

Examples:
    agentbox                                          # Start Claude CLI for current project
    agentbox shell                                    # Start interactive shell instead of Claude
    agentbox shell --admin                            # Start admin shell with sudo access
    agentbox --add-dir ~/proj1, ~/proj2               # Add directories with Claude CLI
    agentbox python script.py                         # Run Python script in container
    agentbox --cleanup                                # Remove image and optionally cached data
    agentbox --rebuild                                # Force rebuild image
    agentbox ssh-init                                 # Set up SSH for AgentBox

Containers are ephemeral and automatically removed when you exit.
Package caches and shell history persist between sessions.

Additional workspaces are mounted at /<name> (e.g., /foo, /bar).
The current directory is always mounted as /workspace.
EOF
}

# Remove AgentBox images and cached data
cleanup_all() {
    log_warning "Removing AgentBox images and cached data..."

    # Remove AgentBox image
    if docker image inspect "$IMAGE_NAME" &> /dev/null; then
        log_info "Removing AgentBox image"
        docker rmi "$IMAGE_NAME" &> /dev/null || true
    fi

    # Remove cached data (optional - ask user)
    local cache_base="${HOME}/.cache/agentbox"
    local project_base="${HOME}/.agentbox/projects"

    if [[ -d "$cache_base" ]] || [[ -d "$project_base" ]]; then
        log_warning "This will also remove cached packages and shell history."
        echo -n "Remove cached data? (y/N): "
        read -r response
        if [[ "$response" =~ ^[Yy]$ ]]; then
            [[ -d "$cache_base" ]] && rm -rf "$cache_base"
            [[ -d "$project_base" ]] && rm -rf "$project_base"
            log_success "Cached data removed"
        else
            log_info "Cached data preserved"
        fi
    fi

    log_success "Cleanup complete"
}

# Set up dedicated SSH directory for AgentBox
ssh_setup() {
    local agentbox_ssh="${HOME}/.agentbox/ssh"

    log_info "Setting up AgentBox SSH directory..."
    mkdir -p "${agentbox_ssh}"
    chmod 700 "${agentbox_ssh}"


    # Copy known_hosts if it exists
    if [[ -f "${HOME}/.ssh/known_hosts" ]]; then
        cp "${HOME}/.ssh/known_hosts" "${agentbox_ssh}/known_hosts"
        chmod 600 "${agentbox_ssh}/known_hosts"
        log_success "Copied known_hosts from ~/.ssh"
    fi

    # Check if AgentBox key already exists
    if [[ -f "${agentbox_ssh}/id_ed25519" ]]; then
        log_info "AgentBox SSH key already exists"
    else
        log_info "Generating dedicated SSH key for AgentBox..."
        ssh-keygen -t ed25519 -f "${agentbox_ssh}/id_ed25519" -C "agentbox@$(hostname)" -N ""
        log_success "Generated new SSH key: ${agentbox_ssh}/id_ed25519"
        echo ""
        log_info "Add this public key to your Git provider:"
        echo ""
        cat "${agentbox_ssh}/id_ed25519.pub"
        echo ""
        log_info "Alternatively, replace the keys in ${agentbox_ssh}/ with your desired keys"
    fi

    log_success "SSH setup complete! Directory: ${agentbox_ssh}"
    log_info "AgentBox will use this directory for all SSH operations."
}

# Main execution
main() {
    # Parse arguments
    local force_rebuild=false
    local cleanup_all=false
    local shell_mode=false
    local admin_mode=false
    local cmd_args=()
    local extra_workspaces=()

    while [[ $# -gt 0 ]]; do
        case "$1" in
            -h|--help)
                show_help
                exit 0
                ;;
            --cleanup)
                cleanup_all=true
                shift
                ;;
            --rebuild)
                force_rebuild=true
                shift
                ;;
            --add-dir)
                shift
                while [[ $# -gt 0 && ! "$1" =~ ^- && "$1" != shell ]]; do
                    parse_workspace_paths "$1" extra_workspaces
                    shift
                done
                ;;
            shell)
                shell_mode=true
                shift
                # Check if next arg is --admin
                if [[ "${1:-}" == "--admin" ]]; then
                    admin_mode=true
                    shift
                fi
                # Remaining args become shell commands
                cmd_args=("$@")
                break
                ;;
            ssh-init)
                ssh_setup
                exit 0
                ;;
            *)
                cmd_args+=("$1")
                shift
                ;;
        esac
    done

    # Check Docker
    check_docker

    # Handle special commands
    if [[ "$cleanup_all" == "true" ]]; then
        cleanup_all
        exit 0
    fi

    # Get container name for current project
    local base_container_name=$(get_container_name)
    local instance_suffix=$(get_next_instance)

    # Check if rebuild is needed or forced
    if [[ "$force_rebuild" == "true" ]] || needs_rebuild; then
        if [[ "$force_rebuild" == "true" ]]; then
            log_info "Forcing image rebuild..."
        else
            log_info "Dockerfile or entrypoint changed, rebuilding automatically..."
        fi

        if ! build_image; then
            log_error "Build failed!"
            exit 1
        fi

    fi

    # Run container with appropriate mode
    if [[ "$shell_mode" == "true" ]]; then
        if [[ "$admin_mode" == "true" ]]; then
            run_container "$base_container_name" "$instance_suffix" extra_workspaces "true" "true" "${cmd_args[@]}"
        else
            run_container "$base_container_name" "$instance_suffix" extra_workspaces "true" "false" "${cmd_args[@]}"
        fi
    else
        run_container "$base_container_name" "$instance_suffix" extra_workspaces "false" "false" "${cmd_args[@]}"
    fi
}

# Run main function
main "$@"
